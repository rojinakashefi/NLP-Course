{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drKiwgo7QMoc",
    "outputId": "47546506-6f8a-478d-8a26-db6f05413147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3LFIb_4G3ql"
   },
   "source": [
    "### This video covers:\n",
    "\n",
    "\n",
    "*   Implementation of TF-IDF representation\n",
    "*   Word2vec representation by gensim library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJppNRwPIZBy"
   },
   "source": [
    "### part 1: Implementation of TF-IDF representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhzMxF5DHJ34"
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1F4gXB_YhEKyJrQjtZ5e7UvzapsfhW1lR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1lU5MBuk7Ap"
   },
   "outputs": [],
   "source": [
    "# this function reads the 'Hamshahri' corpus and returns its documents (docs) and document Ids (DIDs)\n",
    "def read_document(path):\n",
    "\n",
    "  res = []\n",
    "\n",
    "  with open(path, 'r', encoding = 'utf-8') as ptr:\n",
    "    for line in ptr:\n",
    "      res.append(line)\n",
    "\n",
    "  tmp = ' '\n",
    "  docs = []\n",
    "  DIDs = []\n",
    "  DIDs.append(res[0].split()[1])\n",
    "\n",
    "  for i in range(1, len(res)):\n",
    "\n",
    "    if res[i].startswith(\".DID\"):\n",
    "      docs.append(tmp)\n",
    "      tmp = ''\n",
    "      DIDs.append(res[i].split()[1])\n",
    "      continue\n",
    "\n",
    "    if res[i].startswith(\".Date\"):\n",
    "      continue\n",
    "\n",
    "    if res[i].startswith(\".Cat\"):\n",
    "      continue\n",
    "\n",
    "    tmp = tmp+ ' ' + res[i].strip()\n",
    "\n",
    "  docs.append(tmp)\n",
    "\n",
    "  return docs, DIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQwE0_sEmzt4"
   },
   "outputs": [],
   "source": [
    "# this function removes the punctuation marks and non-alphabetic words from given document \n",
    "# and returns an array of document words\n",
    "def clean_text(doc):\n",
    "\n",
    "  tokens = doc.split()\n",
    "  translation_table = str.maketrans('', '', \"><.،؟؛:{}\\|+ـ()*&^٪$#❊!/[]=-\")\n",
    "  tokens = [word.translate(translation_table) for word in tokens]\n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqZ41prEl9an"
   },
   "outputs": [],
   "source": [
    "docs, DIDs = read_document('/content/drive/MyDrive/dataset/Hamshahri-Corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81WZ9XuAuwJw"
   },
   "outputs": [],
   "source": [
    "N = len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XhLor3wxEgW"
   },
   "outputs": [],
   "source": [
    "\n",
    "# this function creates two term_doc, and doc_term dictionaries\n",
    "def create_tf_idf_dicts(docs, DIDs):\n",
    "  \n",
    "  # [key = T (term)] --> dict[key = D (doc)] --> number of times the term T appeared in doc D\n",
    "  term_doc = {}\n",
    "\n",
    "  # [key = D (doc)] --> dict[key = T (term)] --> number of times the term T appeared in doc D\n",
    "  doc_term = {}\n",
    "\n",
    "  for doc, DID in zip(docs, DIDs):\n",
    "    terms = clean_text(doc)\n",
    "    doc_term[DID] = {}\n",
    "\n",
    "    for term in terms:\n",
    "   \n",
    "  # update doc_term[DID] dictionary\n",
    "\n",
    "      # current term has appeared in document 'DID' in orevious steps:\n",
    "      if term in doc_term[DID]:\n",
    "        doc_term[DID][term] += 1\n",
    "      \n",
    "      # this is the first occurrance of the current term in document 'DID':\n",
    "      else: \n",
    "        doc_term[DID][term] = 1\n",
    "\n",
    "  # update term_doc[term] dictionary\n",
    "\n",
    "      if not term in term_doc:\n",
    "        term_doc[term] = {}\n",
    "\n",
    "      # current term has appeared in document DID in previous steps:\n",
    "      if DID in term_doc[term]:\n",
    "          term_doc[term][DID] +=1\n",
    "      \n",
    "      # this is the first time the current term appears in document DID:\n",
    "      else: \n",
    "        term_doc[term][DID] = 1     \n",
    "\n",
    "  return term_doc, doc_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_Ih9ZDroUFj"
   },
   "outputs": [],
   "source": [
    "term_doc, doc_term = create_tf_idf_dicts(docs, DIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjgERWV9zVtA"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# this function calculates the TF-IDF weight of input term within the input document\n",
    "def get_tf_idf(term, doc):\n",
    "  \n",
    "  count_t_d = 0\n",
    "  \n",
    "  if doc in term_doc[term]:\n",
    "    count_t_d = term_doc[term][doc]\n",
    "    tf = 1 + math.log(count_t_d, 10)\n",
    "  \n",
    "  idf = get_IDF(term)\n",
    "\n",
    "  return tf*idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dlr2US-43xaJ"
   },
   "outputs": [],
   "source": [
    "# this function calculates the IDF weight of input term\n",
    "def get_IDF(term):\n",
    "  \n",
    "  df = 0\n",
    "  \n",
    "  for doc in doc_term:\n",
    "    if term in doc_term[doc]:\n",
    "      df += 1\n",
    "  \n",
    "  if df>0:\n",
    "    idf = math.log((N/df), 2)\n",
    "  \n",
    "  return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOsT9UGItSl5",
    "outputId": "ba371283-a8a8-4090-cf17-13bfdc1cc10d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8190675501480003"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_IDF('تهران')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUHZNjov2-WG"
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1BpsCwRd2cee7lGHWXarOrtCxKu8a-f3B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzkUL_Kv31IG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# this function claculates the cosine similarity of the input documents \n",
    "def cosine_sim_of_docs(doc_1, doc_2):\n",
    "  \n",
    "  if not (doc_1 in doc_term and doc_2 in doc_term):\n",
    "    print('Invalid document number')\n",
    "    return 0\n",
    "\n",
    "  dot_product = 0\n",
    "  \n",
    "  tf_doc_1 = doc_term[doc_1]\n",
    "  tf_doc_2 = doc_term[doc_2]\n",
    "\n",
    "  # compute the dot product of two document representations\n",
    "  for term in tf_doc_1:   \n",
    "    if term in tf_doc_2:\n",
    "\n",
    "      dot_product += get_tf_idf(term, doc_1) *  get_tf_idf(term, doc_2) \n",
    "  \n",
    "  length_doc_1 = 0\n",
    "  length_doc_2 = 0\n",
    "\n",
    "  # compute the length of doc_1's representation\n",
    "  for term in tf_doc_1:\n",
    "    length_doc_1 += get_tf_idf(term, doc_1) * get_tf_idf(term, doc_1)\n",
    "\n",
    "  # compute the length of doc_2's representation\n",
    "  for term in tf_doc_2:\n",
    "    length_doc_2 += get_tf_idf(term, doc_2) * get_tf_idf(term, doc_2)\n",
    "\n",
    "  # calculate the cosine similarity od doc_1 and doc_2\n",
    "  cosine_sim = dot_product / (np.sqrt(length_doc_1) * np.sqrt(length_doc_2))\n",
    "\n",
    "  return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKkoshogRZqg",
    "outputId": "3a235706-6949-41e2-945c-4cee6bf5e713"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01656446971653419"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim_of_docs('244S1', '279S1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MD6O4GfxSAAU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# this function claculates the cosine similarity of the input term \n",
    "def cosine_sim_of_words(term_1, term_2):\n",
    "\n",
    "  if not (term_1 in term_doc and term_2 in term_doc):\n",
    "    print('out of vocab word!')\n",
    "    return 0\n",
    "\n",
    "  dot_product = 0\n",
    "  \n",
    "  doc_freq_term_1 = term_doc[term_1]\n",
    "  doc_freq_term_2 = term_doc[term_2]\n",
    "\n",
    "  idf_term_1 = get_IDF(term_1)\n",
    "  idf_term_2 = get_IDF(term_2)\n",
    "  \n",
    "  # compute the dot product of two term representations\n",
    "  for doc in doc_freq_term_1:   \n",
    "    if doc in doc_freq_term_2:\n",
    "      dot_product += get_tf_idf(term_1, doc) * get_tf_idf(term_2, doc) \n",
    "  \n",
    "  length_doc_1 = 0\n",
    "  length_doc_2 = 0\n",
    "\n",
    "\n",
    "  # compute the length of term_1's representation\n",
    "  for doc in doc_freq_term_1:\n",
    "    length_doc_1 += get_tf_idf(term_1, doc) * get_tf_idf(term_1, doc)\n",
    "\n",
    "  # compute the length of term_2's representation\n",
    "  for doc in doc_freq_term_2:\n",
    "    length_doc_2 += get_tf_idf(term_2, doc) * get_tf_idf(term_2, doc)\n",
    "\n",
    "  # calculate the cosine similarity od term_1 and term_2\n",
    "  cosine_sim = dot_product / (np.sqrt(length_doc_1) * np.sqrt(length_doc_2))\n",
    "\n",
    "  return cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZweJhRo2Iji3"
   },
   "source": [
    "### part 2: Word2vec by Gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voHGSExomE39"
   },
   "outputs": [],
   "source": [
    "# create input for gensim word2vec model\n",
    "\n",
    "sentences = []\n",
    "for doc in docs:\n",
    "  sentences.append(clean_text(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeE3FMcrpgt9"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# sg: Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "# min_count: Ignores all words with total frequency lower than this.\n",
    "# window: Maximum distance between the current and predicted word within a sentence.\n",
    "# size: Dimensionality of the word vectors.\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1, size=100, window=10, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tRNT_xRu1Hd"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(\"/content/drive/MyDrive/models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjcOkplBEJBd"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# load the model\n",
    "model = Word2Vec.load(\"/content/drive/MyDrive/models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbMY9EZSuyAx",
    "outputId": "c8134149-f71d-4e8c-842a-b926c9ec3843"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.train([[\"همشهری\", \"روزنامه\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yydV8p-Nu4jn"
   },
   "outputs": [],
   "source": [
    "# get vector representation of each term\n",
    "vector = model.wv['خيابان'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0IAIphsFM_E",
    "outputId": "d80c901c-b6d4-4b01-d76f-c4a7f9350236"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.7829500e-01, -3.3936438e-01,  8.2576412e-01,  3.7145108e-01,\n",
       "        6.7576092e-01,  5.8686972e-02,  4.8742581e-02,  1.2406912e-01,\n",
       "       -4.3891722e-01,  7.0365928e-02, -3.4964731e-01, -1.3779813e-01,\n",
       "        1.6468327e-01,  2.4502856e-01,  3.5093248e-01, -3.2618856e-01,\n",
       "        5.9894842e-01,  1.0751355e+00,  1.6326882e-02, -1.5088303e-01,\n",
       "        1.6317840e-01, -1.2954479e-01,  3.6789209e-01, -5.5537474e-01,\n",
       "        1.9381578e-01, -1.3732421e-01, -3.4485805e-01,  1.7315307e-01,\n",
       "        3.7723914e-01, -6.6332656e-01,  3.6547026e-01,  3.6495346e-01,\n",
       "        2.6299605e-01,  2.7699494e-01,  5.5166113e-01, -3.7951469e-02,\n",
       "        2.4369428e-02, -1.6798066e-01,  4.6101886e-01, -3.3071759e-01,\n",
       "       -8.0350757e-01, -1.4511347e-01, -4.4636783e-01, -8.4169728e-01,\n",
       "        3.7407890e-01, -6.7757732e-01,  3.2781810e-02,  1.1740174e-01,\n",
       "       -3.2059025e-02,  2.0361900e-01,  1.8284015e-02,  5.0743729e-01,\n",
       "       -7.9127812e-01,  1.4968026e-01, -3.1331035e-01,  6.5365329e-02,\n",
       "        2.8309339e-01,  1.2984358e-01, -3.6233533e-02, -1.2127010e-02,\n",
       "        5.6021649e-01,  4.0005535e-01,  3.3604065e-01, -3.9629486e-01,\n",
       "        4.0788096e-01, -1.4861806e-01,  2.9002714e-01, -8.6864299e-01,\n",
       "        8.8348401e-01,  2.9378653e-01,  2.3446381e-01, -1.9823499e-01,\n",
       "       -1.2798081e-03,  3.5602266e-01, -6.1617970e-01,  1.6538659e-01,\n",
       "        5.4843850e-02,  2.8382158e-01,  2.2982329e-01,  5.2924216e-01,\n",
       "        4.5817807e-01, -9.2289150e-02,  5.4847383e-01,  7.2766954e-01,\n",
       "        1.4930275e-01,  1.4327475e-01,  4.1433772e-01, -4.6661693e-01,\n",
       "        8.5197367e-02,  5.9736794e-01,  9.4620623e-02, -1.5131103e-01,\n",
       "       -1.2676959e-01,  7.9920685e-01, -4.1024169e-01,  2.7905273e-01,\n",
       "       -3.1543005e-01,  1.0854067e-02, -8.6887115e-01, -1.4068953e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWjU4Y7r8F3-",
    "outputId": "96da46ea-d3c4-4e9d-bc36-5b3c60fa6d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('چهارراه', 0.8367165327072144), ('وليعصر', 0.8330073356628418), ('كوچه', 0.8281815052032471), ('ميرداماد', 0.8225080370903015), ('ضلع', 0.8036145567893982)]\n"
     ]
    }
   ],
   "source": [
    "# get the most similar term to the input term\n",
    "sims = model.wv.most_similar('خيابان', topn=5)\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLwsu7GZ8HhS",
    "outputId": "dd2d0e83-2467-454a-87e5-084928474d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37874202264156015\n",
      "0.287817732181497\n",
      "0.2704187837993078\n",
      "0.25563191693756854\n",
      "0.19136493917027125\n"
     ]
    }
   ],
   "source": [
    "# get the cosine similarity of TF-IDF representations\n",
    "\n",
    "print(cosine_sim_of_words('خيابان', 'كوچه'))\n",
    "print(cosine_sim_of_words('خيابان', 'چهارراه'))\n",
    "print(cosine_sim_of_words('خيابان', 'وليعصر'))\n",
    "print(cosine_sim_of_words('خيابان', 'مطهري'))\n",
    "print(cosine_sim_of_words('خيابان', 'تقاطع'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF_IDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
